@misc{Stefan2018,
  author = {Stefan T. Ruehl},
  title = {Latex Template for Research Proposals},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/stefantruehl/research-proposal-template}},
  commit = {master}
}

@inproceedings{snoek2012practical,
  title={{Practical Bayesian optimization of machine learning algorithms}},
  author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  booktitle=NIPS,
  pages={2951--2959},
  year={2012}
}

@article{mockus1994application,
  title={{Application of Bayesian approach to numerical methods of global and stochastic optimization}},
  author={Mockus, Jonas},
  journal={Journal of Global Optimization},
  volume={4},
  number={4},
  pages={347--365},
  year={1994},
  publisher={Springer}
}

@book{williams2006gaussian,
  title={Gaussian processes for machine learning},
  author={Williams, Christopher KI and Rasmussen, Carl Edward},
  volume={2},
  number={3},
  year={2006},
  publisher={MIT press Cambridge, MA}
}

@article{chen2018bayesian,
  title={Bayesian optimization in alphago},
  author={Chen, Yutian and Huang, Aja and Wang, Ziyu and Antonoglou, Ioannis and Schrittwieser, Julian and Silver, David and de Freitas, Nando},
  journal={arXiv preprint arXiv:1812.06855},
  year={2018}
}

@inproceedings{wang2013bayesian,
  title={{Bayesian optimization in high dimensions via random embeddings}},
  author={Wang, Ziyu and Zoghi, Masrour and Hutter, Frank and Matheson, David and De Freitas, Nando},
  booktitle=IJCAI,
  year={2013}
}

@ARTICLE{7352306,
  author={Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and de Freitas, Nando},
  journal={Proceedings of the IEEE}, 
  title={Taking the Human Out of the Loop: A Review of Bayesian Optimization}, 
  year={2016},
  volume={104},
  number={1},
  pages={148-175},
  doi={10.1109/JPROC.2015.2494218}}

@article{ambikasaran2015fast,
  title={Fast direct methods for Gaussian processes},
  author={Ambikasaran, Sivaram and Foreman-Mackey, Daniel and Greengard, Leslie and Hogg, David W and Oâ€™Neil, Michael},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={38},
  number={2},
  pages={252--265},
  year={2015},
  publisher={IEEE}
}
@inproceedings{chen2012joint,
  title={{Joint optimization and variable selection of high-dimensional Gaussian processes}},
  author={Chen, Bo and Castro, Rui M and Krause, Andreas},
  booktitle=ICML,
  pages={1379--1386},
  year={2012}
}
@inproceedings{kandasamy2015high,
  title={{High dimensional Bayesian optimisation and bandits via additive models}},
  author={Kandasamy, Kirthevasan and Schneider, Jeff and P{\'o}czos, Barnab{\'a}s},
  booktitle=ICML,
  pages={295--304},
  year={2015}
}

@inproceedings{hutter2013evaluation,
  title={An evaluation of sequential model-based optimization for expensive blackbox functions},
  author={Hutter, Frank and Hoos, Holger and Leyton-Brown, Kevin},
  booktitle={Proceedings of the 15th annual conference companion on Genetic and evolutionary computation},
  pages={1209--1216},
  year={2013}
}
@article{oh2019combinatorial,
  title={Combinatorial bayesian optimization using the graph cartesian product},
  author={Oh, Changyong and Tomczak, Jakub M and Gavves, Efstratios and Welling, Max},
  journal={arXiv preprint arXiv:1902.00448},
  year={2019}
}

@article{daxberger2019mixed,
  title={Mixed-variable bayesian optimization},
  author={Daxberger, Erik and Makarova, Anastasia and Turchetta, Matteo and Krause, Andreas},
  journal={arXiv preprint arXiv:1907.01329},
  year={2019}
}
@inproceedings{hutter2011sequential,
  title={Sequential model-based optimization for general algorithm configuration},
  author={Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
  booktitle={International conference on learning and intelligent optimization},
  pages={507--523},
  year={2011},
  organization={Springer}
}

@techreport{seeger2003fast,
  title={Fast forward selection to speed up sparse Gaussian process regression},
  author={Seeger, Matthias and Williams, Christopher and Lawrence, Neil},
  year={2003}
}

@article{han2020high,
  title={High-Dimensional Bayesian Optimization via Tree-Structured Additive Models},
  author={Han, Eric and Arora, Ishank and Scarlett, Jonathan},
  journal={Proc. of AAAI Conference on Artificial Intelligence},
  year={2021}
}

@misc{NeurIPS2017,
  title = {NeurIPS 2017 Schedule},
  author={NeurIPS},
  howpublished = {\url{https://nips.cc/Conferences/2017/Schedule?showEvent=8773}},
  note = {Accessed: 2020-Apr-28},
  year={2017}
}
@book{bengio2009learning,
  title={Learning deep architectures for AI},
  author={Bengio, Yoshua},
  year={2009},
  publisher={Now Publishers Inc}
}
@inproceedings{baptista2018bayesian,
  title={Bayesian optimization of combinatorial structures},
  author={Baptista, Ricardo and Poloczek, Matthias},
  booktitle={International Conference on Machine Learning},
  pages={462--471},
  year={2018},
  organization={PMLR}
}

@InProceedings{pmlr-v124-swersky20a, title = {Amortized Bayesian Optimization over Discrete Spaces}, author = {Swersky, Kevin and Rubanova, Yulia and Dohan, David and Murphy, Kevin}, booktitle = {Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI)}, pages = {769--778}, year = {2020}, editor = {Jonas Peters and David Sontag}, volume = {124}, series = {Proceedings of Machine Learning Research}, month = {03--06 Aug}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v124/swersky20a/swersky20a.pdf}, url = { http://proceedings.mlr.press/v124/swersky20a.html }, abstract = {Bayesian optimization is a principled approach for globally optimizing expensive, black-box functions by using a surrogate model of the objective. However, each step of Bayesian optimization involves solving an inner optimization problem, in which we maximize an acquisition function derived from the surrogate model to decide where to query next. This inner problem can be challenging to solve, particularly in discrete spaces, such as protein sequences or molecular graphs, where gradient-based optimization cannot be used. Our key insight is that we can train a generative model to generate candidates that maximize the acquisition function. This is faster than standard model-free local search methods, since we can amortize the cost of learning the model across multiple rounds of Bayesian optimization. We therefore call this Amortized Bayesian Optimization. On several challenging discrete design problems, we show this method generally outperforms other methods at optimizing the inner acquisition function, resulting in more efficient optimization of the outer black-box objective.} }

@InProceedings{pmlr-v70-jenatton17a, title = {{B}ayesian Optimization with Tree-structured Dependencies}, author = {Rodolphe Jenatton and Cedric Archambeau and Javier Gonz{\'a}lez and Matthias Seeger}, booktitle = {Proceedings of the 34th International Conference on Machine Learning}, pages = {1655--1664}, year = {2017}, editor = {Precup, Doina and Teh, Yee Whye}, volume = {70}, series = {Proceedings of Machine Learning Research}, month = {06--11 Aug}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v70/jenatton17a/jenatton17a.pdf}, url = { http://proceedings.mlr.press/v70/jenatton17a.html }, abstract = {Bayesian optimization has been successfully used to optimize complex black-box functions whose evaluations are expensive. In many applications, like in deep learning and predictive analytics, the optimization domain is itself complex and structured. In this work, we focus on use cases where this domain exhibits a known dependency structure. The benefit of leveraging this structure is twofold: we explore the search space more efficiently and posterior inference scales more favorably with the number of observations than Gaussian Process-based approaches published in the literature. We introduce a novel surrogate model for Bayesian optimization which combines independent Gaussian Processes with a linear model that encodes a tree-based dependency structure and can transfer information between overlapping decision sequences. We also design a specialized two-step acquisition function that explores the search space more effectively. Our experiments on synthetic tree-structured functions and the tuning of feedforward neural networks trained on a range of binary classification datasets show that our method compares favorably with competing approaches.} }